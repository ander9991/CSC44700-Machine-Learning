{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "from matplotlib.backends import backend_agg\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris.csv', header = None)\n",
    "\n",
    "# Getting features\n",
    "iris_features = data.iloc[:,:4]\n",
    "\n",
    "# Getting species \n",
    "iris_classes = data.iloc[:,4]\n",
    "\n",
    "# The 6 possible combinations of 2 features\n",
    "features = [[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]]\n",
    "feature_titles = [\"AB\",\"AC\",\"AD\",\"BC\",\"BD\",\"CD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent Functions\n",
    "def SGD(selected_features): # SGD (data not scaled)\n",
    "    SGD = SGDClassifier(loss=\"modified_huber\")\n",
    "    SGD.fit(selected_features, iris_classes)\n",
    "    return SGD.score(selected_features, iris_classes)\n",
    "\n",
    "def SGD_scaled(selected_features): # SGD (scaled data)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(selected_features)\n",
    "    data = scaler.transform(selected_features)\n",
    "    \n",
    "    SGD = SGDClassifier(loss=\"modified_huber\")\n",
    "    SGD.fit(data, iris_classes)\n",
    "\n",
    "    return SGD.score(data, iris_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Functions\n",
    "def LogisticReg(selected_features): # Logistic regresssion (data not scaled)\n",
    "    LogReg = LogisticRegression(random_state = 0)\n",
    "    LogReg.fit(selected_features, iris_classes)\n",
    "    \n",
    "    return LogReg.score(selected_features, iris_classes)\n",
    "\n",
    "def LogisticReg_scaled(selected_features): # Logistic regression (scaled data)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(selected_features)\n",
    "    data = scaler.transform(selected_features)\n",
    "    \n",
    "    LogReg = LogisticRegression(random_state = 0)\n",
    "    LogReg.fit(data, iris_classes)\n",
    "    \n",
    "    return LogReg.score(data, iris_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Functions\n",
    "\n",
    "def SVM(selected_features, kernel): # SVM (data not scaled)\n",
    "    svm = SVC(kernel = kernel)\n",
    "    svm.fit(selected_features, iris_classes)\n",
    "    \n",
    "    return svm.score(selected_features, iris_classes)\n",
    "\n",
    "def SVM_scaled(selected_features, kernel): # SVM (scaled data)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(selected_features)\n",
    "    data = scaler.transform(selected_features)\n",
    "    \n",
    "    svm = SVC(kernel = kernel)\n",
    "    svm.fit(data, iris_classes)\n",
    "    \n",
    "    return svm.score(data, iris_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent using the Modified-Huber Loss Function\n",
    "\n",
    "Stochastic gradient descent is a method for finding the optimal parameter configuration for a machine learning model. The process  makes small adjustments in the model to minimize the error. The modified huber loss function isn't sensitive to existing outliers. \n",
    "\n",
    "Since the score changes after every iteration, I ran each function 1000 times and computed the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy results for every 2 feature combination for SGD\n",
    "SGD_Scores = [0]*6\n",
    "Scaled_SGD_Scores = [0]*6\n",
    "\n",
    "for i in range (6):\n",
    "    selected_features = iris_features[features[i]]\n",
    "    \n",
    "    # SGD scores (no scaling)\n",
    "    for j in range (1000):\n",
    "        SGD_Scores[i] += SGD(selected_features)\n",
    "    SGD_Scores[i] /= 1000\n",
    "    \n",
    "    # SGD scores (features scaled)\n",
    "    for j in range (1000):\n",
    "        Scaled_SGD_Scores[i] += SGD_scaled(selected_features)\n",
    "    Scaled_SGD_Scores[i] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic gradient descent results (No scaling):\n",
      "Features AB results: 0.6967999999999975\n",
      "Features AC results: 0.7873399999999973\n",
      "Features AD results: 0.7649866666666664\n",
      "Features BC results: 0.8362800000000017\n",
      "Features BD results: 0.8749800000000032\n",
      "Features CD results: 0.8514400000000009\n",
      "\n",
      "Stochastic gradient descent results (Scaled data):\n",
      "Features AB results: 0.7492933333333298\n",
      "Features AC results: 0.9259733333333372\n",
      "Features AD results: 0.9048133333333376\n",
      "Features BC results: 0.918766666666666\n",
      "Features BD results: 0.9337200000000087\n",
      "Features CD results: 0.9343800000000088\n"
     ]
    }
   ],
   "source": [
    "# SGD results\n",
    "print(\"Stochastic gradient descent results (No scaling):\")\n",
    "for i in range(6):\n",
    "    print(\"Features\",feature_titles[i],\"results:\",SGD_Scores[i])\n",
    "\n",
    "print(\"\\nStochastic gradient descent results (Scaled data):\")\n",
    "for i in range(6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", Scaled_SGD_Scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is an example of a classification supervised machine learning model. Similar to the SGD, I will obtain the scores with and without scaling.\n",
    "\n",
    "Since the accuracies don't vary after each iteration like the SGD, computating the average accuracy is redundant. Furthermore, logistic regression isn't too sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy results for every 2 feature combination for Logistic Regression\n",
    "LogRegScores = []\n",
    "LogRegScaledScores = []\n",
    "\n",
    "for i in range (6):\n",
    "    selected_features = iris_features[features[i]]\n",
    "    \n",
    "    # logistic regression scores (no scaling)\n",
    "    LogRegScores.append(LogisticReg(selected_features))\n",
    "    \n",
    "    # logistic regression scores (features scaled)\n",
    "    LogRegScaledScores.append(LogisticReg_scaled(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression results (no scaling):\n",
      "Features AB results: 0.82\n",
      "Features AC results: 0.96\n",
      "Features AD results: 0.96\n",
      "Features BC results: 0.9533333333333334\n",
      "Features BD results: 0.96\n",
      "Features CD results: 0.9666666666666667\n",
      "\n",
      "Logistic regression results (scaled data):\n",
      "Features AB results: 0.8133333333333334\n",
      "Features AC results: 0.9533333333333334\n",
      "Features AD results: 0.96\n",
      "Features BC results: 0.9533333333333334\n",
      "Features BD results: 0.9533333333333334\n",
      "Features CD results: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression results\n",
    "print(\"Logistic regression results (no scaling):\")\n",
    "for i in range (6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", LogRegScores[i])\n",
    "\n",
    "print(\"\\nLogistic regression results (scaled data):\")\n",
    "for i in range(6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", LogRegScaledScores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines is another machine learning model that is suppervised that can perform regression, classification, and outlier detection. It is effective in high dimensional spaces.\n",
    "\n",
    "Linear Kernal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy results for every 2 feature combination for SVM where kernel is linear\n",
    "Linear_SVM_Scores = []\n",
    "Scaled_Linear_SVM_Scores = []\n",
    "\n",
    "for i in range (6):\n",
    "    selected_features = iris_features[features[i]]\n",
    "    \n",
    "    # logistic regression scores (no scaling)\n",
    "    Linear_SVM_Scores.append(SVM(selected_features, \"linear\"))\n",
    "    \n",
    "    # logistic regression scores (features scaled)\n",
    "    Scaled_Linear_SVM_Scores.append(SVM_scaled(selected_features, \"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with a Linear Kernal (Nonscaled) results:\n",
      "Features AB results: 0.82\n",
      "Features AC results: 0.9533333333333334\n",
      "Features AD results: 0.96\n",
      "Features BC results: 0.96\n",
      "Features BD results: 0.96\n",
      "Features CD results: 0.9666666666666667\n",
      "\n",
      "SVM with a Linear Kernal (Scaled) results:\n",
      "Features AB results: 0.8133333333333334\n",
      "Features AC results: 0.9733333333333334\n",
      "Features AD results: 0.96\n",
      "Features BC results: 0.9533333333333334\n",
      "Features BD results: 0.9533333333333334\n",
      "Features CD results: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"SVM with a Linear Kernal (Nonscaled) results:\")\n",
    "for i in range (6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", Linear_SVM_Scores[i])\n",
    "\n",
    "print(\"\\nSVM with a Linear Kernal (Scaled) results:\")\n",
    "for i in range(6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", Scaled_Linear_SVM_Scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernal = Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy results for every 2 feature combination for SVM where kernel is Polynomial\n",
    "Polynomial_SVM_Scores = []\n",
    "Scaled_Polynomial_SVM_Scores = []\n",
    "\n",
    "for i in range (6):\n",
    "    selected_features = iris_features[features[i]]\n",
    "    \n",
    "    # logistic regression scores (no scaling)\n",
    "    Polynomial_SVM_Scores.append(SVM(selected_features, \"poly\"))\n",
    "    \n",
    "    # logistic regression scores (features scaled)\n",
    "    Scaled_Polynomial_SVM_Scores.append(SVM_scaled(selected_features, \"poly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine (Kernel = polynomial) results (no scaling):\n",
      "Features AB results: 0.8133333333333334\n",
      "Features AC results: 0.96\n",
      "Features AD results: 0.9533333333333334\n",
      "Features BC results: 0.9533333333333334\n",
      "Features BD results: 0.96\n",
      "Features CD results: 0.9666666666666667\n",
      "\n",
      "Support vector machine (Kernel = polynomial) results (scaled data):\n",
      "Features AB results: 0.7466666666666667\n",
      "Features AC results: 0.9\n",
      "Features AD results: 0.9066666666666666\n",
      "Features BC results: 0.9066666666666666\n",
      "Features BD results: 0.92\n",
      "Features CD results: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"SVM with a Polynomial Kernal (Nonscaled) results:\")\n",
    "for i in range (6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", Polynomial_SVM_Scores[i])\n",
    "\n",
    "print(\"\\nSVM with a Polynomial Kernal (Scaled) results:\")\n",
    "for i in range(6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", Scaled_Polynomial_SVM_Scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernal = RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy results for every 2 feature combination for SVM where kernel is RB\n",
    "RBF_SVM_Scores = []\n",
    "Scaled_RBF_SVM_Scores = []\n",
    "\n",
    "for i in range (6):\n",
    "    selected_features = iris_features[features[i]]\n",
    "    \n",
    "    # getting logistic regression scores (no scaling)\n",
    "    RBF_SVM_Scores.append(SVM(selected_features, \"rbf\"))\n",
    "    \n",
    "    # getting logistic regression scores (features scaled)\n",
    "    Scaled_RBF_SVM_Scores.append(SVM_scaled(selected_features, \"rbf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine (Kernel = rbf) results (no scaling):\n",
      "Features AB results: 0.82\n",
      "Features AC results: 0.96\n",
      "Features AD results: 0.96\n",
      "Features BC results: 0.9533333333333334\n",
      "Features BD results: 0.96\n",
      "Features CD results: 0.9533333333333334\n",
      "\n",
      "Support vector machine (Kernel = rbf) results (scaled data):\n",
      "Features AB results: 0.82\n",
      "Features AC results: 0.96\n",
      "Features AD results: 0.96\n",
      "Features BC results: 0.9466666666666667\n",
      "Features BD results: 0.9533333333333334\n",
      "Features CD results: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "print(\"SVM with a RBF Kernal (Nonscaled) results:\")\n",
    "for i in range (6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", RBF_SVM_Scores[i])\n",
    "\n",
    "print(\"\\nSVM with a RBF Kernal (Scaled) results:\")\n",
    "for i in range(6):\n",
    "    print(\"Features\", feature_titles[i], \"results:\", Scaled_RBF_SVM_Scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Results\n",
    "\n",
    "For starters, I scaled the data for each model in order to improve the accuracies ever so slightly.\n",
    "\n",
    "A) sepal length\n",
    "\n",
    "B) sepal width\n",
    "\n",
    "C) petal length\n",
    "\n",
    "D) petal width\n",
    "\n",
    "## SGD\n",
    "\n",
    "### <center>SGD Results:</center>\n",
    "|     | AB | AC | AD | BC | BD | CD | \n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Unscaled Data | 0.6967999999999975 | 0.7873399999999973 | 0.7649866666666664 | 0.8362800000000017 | 0.8749800000000032 | 0.8514400000000009 |\n",
    "| Scaled Data   | 0.7492933333333298  | 0.9259733333333372   | 0.9048133333333376 | 0.918766666666666  | 0.9337200000000087 | 0.9343800000000088 |\n",
    "\n",
    "As mentioned before, the accuracy improves ever so slightly after scaling the data. Furthermore, the results are the average taken from running the model a 1000 times. To improve on the model even more, I could run the model even more than 1000 times. Generally, the accuracies are pretty good with the exception being the feature combination of (sepal length, sepal width).\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "### <center> Logistic Regression Results:</center>\n",
    "|     | AB | AC | AD | BC | BD | CD | \n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Unscaled Data | 0.82               | 0.96               | 0.96 | 0.9533333333333334 | 0.96               | 0.9666666666666667 |\n",
    "| Scaled Data   | 0.8133333333333334 | 0.9533333333333334 | 0.96 | 0.9533333333333334 | 0.9533333333333334 | 0.9533333333333334 |\n",
    "\n",
    "An interesting observation I made is that in some feature combinations, scaling the results worsened the accuracies ever so slightly. This demonstrates that logistic regression can classify the data with ease whether the data is scaled or not.\n",
    "\n",
    "## Support Vector Machines\n",
    "\n",
    "### <center> Linear Results </center>\n",
    "|     | AB | AC | AD | BC | BD | CD | \n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Unscaled Data | 0.82               | 0.9533333333333334 | 0.96 | 0.96               | 0.96               | 0.9666666666666667 |\n",
    "| Scaled Data   | 0.8133333333333334 | 0.9733333333333334 | 0.96 | 0.9533333333333334 | 0.9533333333333334 | 0.9533333333333334 |\n",
    "\n",
    "Almost all feature combinations performed worse after scaling the data, but not by much. AC and Ad ended up staying the same. Scaling the data proved that a linear kernal having a slightly harder time classifying values correctly.\n",
    "\n",
    "### <center> Polynomial Results </center>\n",
    "|     | AB | AC | AD | BC | BD | CD | \n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Unscaled Data | 0.8133333333333334 | 0.96 | 0.9533333333333334 | 0.9533333333333334 | 0.96 | 0.9666666666666667 |\n",
    "| Scaled Data   | 0.7466666666666667 | 0.9  | 0.9066666666666666 | 0.9066666666666666 | 0.92 | 0.96               |\n",
    "\n",
    "The table shows that scaling the data resulted in the Polynomial kernal performing worse then non-scaling the data. \n",
    "\n",
    "### <center> RBF Results </center>\n",
    "|     | AB | AC | AD | BC | BD | CD | \n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| Unscaled Data | 0.82 | 0.96 | 0.96 | 0.9533333333333334 | 0.96 | 0.9533333333333334 |\n",
    "| Scaled Data   | 0.82 | 0.96 | 0.96 | 0.9466666666666667 | 0.9533333333333334 | 0.96 |\n",
    "\n",
    "Similar to the other Support Vector Machine results, scaling the data caused a slight reduction in accuracy. Except for CD, the accuracies either stayed the same or were slightly worse. \n",
    "\n",
    "In conclusion, scaling the data caused the accuracies to stay the same, or to be reduced. However, it was important to scale the data for the Stochastic Gradient Descent and the Logistic regression, as the accuracies improved significantly for SGD, and ever so slightly for the logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab65a6a126614c4d8a09c3bb162b3d2e4f4a949753c6f0f735c7c1fe269df83b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
